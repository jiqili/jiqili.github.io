---
title: 'CPU LLMs'
tags: ['LLM']
---
## 概念
### 量化
把FP32位的数据，通过公式缩放到INT8位的数据
**实际例子：**
```
原始权重: [0.5234, -0.1234, 0.8765, -0.9876]
  ↓ 量化
INT8值:   [127, -31, 223, -251]

存储空间: 16 bytes → 4 bytes (4倍压缩)
```
### 剪枝
把矩阵小于设定的阈值的权重设为0,从而减少计算量

### 知识蒸馏
小模型学习大模型的概率分布，但是层数减少

### 算子融合
有利于减少内存访问。比如计算`max(x *2.0 + 1.0)`,一般会创建两个临时数组，3读3写，而GCC, LLVM和OpenVino等编译器会把这个操作融合成一个算子，临时结果尽量存在CPU的寄存器中，而非内存

例如FMA(Fused Multiply-Add)也是一种算子融合，把乘法和加法融合成一个指令，减少内存访问，中间结果存在硬件内部中，不占用寄存器和内存/缓存
### 常量折叠
比如模型中有固定的一些常量计算，可以预先计算好，减少推理时的计算量

### SIMD单指令多数据
CPU一条指令同时处理多个数据。常见的SIMD指令集有SSE, AVX, AVX-512等。在Ubuntu上，通过`cat /proc/cpuinfo | grep flags`可以查看CPU支持的指令集

NumPy库会自动使用SIMD指令集进行计算，所以使用NumPy进行矩阵运算时会有非常好的性能。使用`numpy.show_config()`可以查看NumPy支持的SIMD指令集。

批处理可以充分利用SIMD的优势，但是batch size太大，内存会有问题，超过并行能力导致收益递减，凑齐batch会有延迟。

### 内存布局优化
CPU从内存连续读取数据加载到缓存中，如果数据是连续存储的，CPU可以预取更多数据到缓存中，减少内存访问次数，提高性能。对于矩阵来说，有些维度是连续存储的，有些维度不是连续存储的。内存布局优化就是把经常一起访问的数据存储在一起，减少内存访问次数，提高缓存命中率。

#### NCHW
NCHW(Batch, Channels, Height, Width): 例如图像`(2, 3, 4, 4)`代表2张图，3通道，4*4像素。
NCHW内存布局适合卷积操作,因为卷积操作通常在空间维度(Height, Width)上进行计算。如果写成for循环，那最外层是图像，第二层是通道，最内层是空间维度。
这样在计算卷积时，可以连续访问空间维度的数据，比如CPU一次读取前16个数据，都是第一张图的R通道的前16个像素，缓存命中率高。

#### NHWC
NHWC则是(Batch, Height, Width, Channels): 例如图像`(2, 4, 4, 3)`代表2张图，4*4像素，3通道。
在处理图像时很合适，因为这些需要读一个完整的RGB转为灰度，或者色彩增强等操作。

但是缺点是同一像素的RGB不连续。
## OpenVino
OpenVino就封装了以上的优化操作，可以把其他平台上的模型格式转成IR格式（XML+bin），但是如果原来的模型有些算子OpenVino不支持的话，就会报错。而且OpenVino官方只支持Intel的CPU和GPU等。

### OpenVino核心技术栈
应用层： openvino-genai(LLM推理框架)
高级API： Optimum-Intel(Transformers集成)
核心引擎：OpenVino Runtime
        图优化器
        算子融合
        内存优化
        调度器
后端插件： CPU Plugin
        AVX-512优化（一个时钟周期内并行计算多个数据）
        INT8/INT4量化
### 量化
#### 对称量化与非对称量化
对称量化：正负范围相等，例如`[-max, max]`, 这样计算更友好，存储也更容易被缩减。
非对称量化，很多张量的分布并不是以0为中心的，`[min, max]`更符合真实情况。
#### 分组量化
每个数值都会被scale成整数或其他类型，可以使用不同的scale进行分组量化，就像使用不同精度的尺子去测量一样。group size越小，精度更好。group size越大，更快。
#### 混合精度量化
ratio控制多少层使用指定的低精度，剩下的层使用更高精度。比如ratio=1就是全部使用低精度，ratio=0.8就是80%的层使用低精度。我们可以控制对精度敏感的层使用高精度，有几种测量。
- 只看int8时的权重误差有多大，误差越小，越要用低精度
- 使用数据集，估计带二阶信息的敏感度。
- 使用数据集，根据输入激活的方差和int8量化的噪声倒数，来估计量化误差对输出的影响。int8量化噪声代表“权重量化引入的误差”。
- 使用数据集，根据输入层激活的平均值和int8量化的噪声倒数，来估计量化误差对输出的影响。
这些策略都在尝试估计某层被压缩到int4时，会造成大多输出偏差。
#### 量化估计
`scale_estimation`，通过分析模型的激活分布，自动选择合适的scale和zero point，从而提高量化后的模型精度。有时候量化前需要准备一小批“代表真实推理分布”的输入数据，用作校准。
而且这个输入不需要`ground truth`标签，因为只是用来估计激活分布的。换句话说，我们只关心“量化前后输出差多少”，而不是“输出的对不对”。
#### 激活感知权重量化
AWQ, Activation Aware Weight Quantization. 有些通道的数值对最终模型输出影响更大，所以量化后可能这些误差会被进一步放大，从而影响最终输出。
AWQ会分析每个通道的激活分布，从而选择合适的量化参数，减少量化误差对最终输出的影响。所以也是需要一小批代表性输入数据来估计激活分布的。
#### 梯度预训练量化
GPTQ, Gradient Post-Training Quantization. 也是需要数据集的，量化时进行误差无偿，来让梯度信息更接近量化前的输出。
#### 是否量化所有层
通常不量化第一层和最后一层，因为这两层对精度影响最大。中间层量化误差会被后续层缓冲掉。
#### LoRA校正
Low-Rank Adaptation. 量化后使用低秩矩阵来校正量化误差，从而提高模型精度。LoRA校正通常需要少量数据进行微调。